{
  "name": "Wan 2.2 (Lightning LoRA)",
  "description": "Workflow for Wan 2.2 video generation (Lightning LoRA)",
  "defaults": {
    "width": 480,
    "height": 800,
    "length": 49,
    "frame_rate": 24,
    "negative_prompt": "色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走"
  },
  "groups": [
    {
      "id": 1,
      "title": "CLIP Configuration",
      "member_nodes": ["1", "note:1"]
    },
    {
      "id": 2,
      "title": "Diffusion Models",
      "member_nodes": ["3", "4", "note:2"]
    },
    {
      "id": 3,
      "title": "Lightning LoRAs",
      "member_nodes": ["8", "9", "note:3"]
    }
  ],
  "notes": [
    {
      "id": 1,
      "title": "CLIP Model Selection",
      "content": "### Recommended CLIP Model\n\nUse **UMT5 XXL FP8 e4m3fn** for optimal text understanding with Wan 2.2 video generation."
    },
    {
      "id": 2,
      "title": "Dual UNET Architecture",
      "content": "### High/Low Noise UNETs\n\nWan 2.2 requires **two separate UNET models** for the diffusion process:\n\n- **High Noise UNET:** Handles the initial denoising steps\n- **Low Noise UNET:** Refines the final output\n\n---\n\n*Both models must be loaded for proper video generation.*"
    },
    {
      "id": 3,
      "title": "Lightning LoRAs",
      "content": "### High/Low Noise Lightning LoRAs\n\nThis workflow uses Lightning LoRAs for fast 4-step inference.\n\n**Two separate LoRAs are required:**\n\n- **High Noise LoRA:** Lightning 4 Steps for initial denoising\n- **Low Noise LoRA:** Lightning 4 Steps for refinement\n\n---\n\n*Both Lightning LoRAs must be loaded for accelerated generation.*"
    }
  ],
  "nodes": {
    "1": {
      "class_type": "CLIPLoader",
      "inputs": {
        "clip_name": "{{clip_name}}",
        "type": "wan",
        "device": "default"
      },
      "_meta": {
        "title": "Load CLIP"
      }
    },
    "2": {
      "class_type": "VAELoader",
      "inputs": {
        "vae_name": "{{vae_name}}"
      },
      "_meta": {
        "title": "Load VAE"
      }
    },
    "3": {
      "class_type": "UNETLoader",
      "inputs": {
        "unet_name": "{{highnoise_unet_name}}",
        "weight_dtype": "default"
      },
      "_meta": {
        "title": "Load Diffusion Model (High Noise)"
      }
    },
    "4": {
      "class_type": "UNETLoader",
      "inputs": {
        "unet_name": "{{lownoise_unet_name}}",
        "weight_dtype": "default"
      },
      "_meta": {
        "title": "Load Diffusion Model (Low Noise)"
      }
    },
    "5": {
      "class_type": "EmptyHunyuanLatentVideo",
      "inputs": {
        "width": "{{width}}",
        "height": "{{height}}",
        "length": "{{length}}",
        "batch_size": "{{batch_size}}"
      },
      "_meta": {
        "title": "Empty HunyuanVideo 1.0 Latent"
      }
    },
    "6": {
      "class_type": "CLIPTextEncode",
      "inputs": {
        "text": "{{positive_prompt}}",
        "clip": ["1", 0]
      },
      "_meta": {
        "title": "CLIP Text Encode (Positive Prompt)"
      }
    },
    "7": {
      "class_type": "CLIPTextEncode",
      "inputs": {
        "text": "{{negative_prompt}}",
        "clip": ["1", 0]
      },
      "_meta": {
        "title": "CLIP Text Encode (Negative Prompt)"
      }
    },
    "8": {
      "class_type": "LoraLoaderModelOnly",
      "inputs": {
        "lora_name": "{{highnoise_lora_name}}",
        "strength_model": 1.0,
        "model": ["3", 0]
      },
      "_meta": {
        "title": "LoraLoaderModelOnly (High Noise)"
      }
    },
    "9": {
      "class_type": "LoraLoaderModelOnly",
      "inputs": {
        "lora_name": "{{lownoise_lora_name}}",
        "strength_model": 1.0,
        "model": ["4", 0]
      },
      "_meta": {
        "title": "LoraLoaderModelOnly (Low Noise)"
      }
    },
    "10": {
      "class_type": "ModelSamplingSD3",
      "inputs": {
        "shift": 5.0,
        "model": ["8", 0]
      },
      "_meta": {
        "title": "ModelSamplingSD3 (High Noise)"
      }
    },
    "11": {
      "class_type": "ModelSamplingSD3",
      "inputs": {
        "shift": 5.0,
        "model": ["9", 0]
      },
      "_meta": {
        "title": "ModelSamplingSD3 (Low Noise)"
      }
    },
    "12": {
      "class_type": "KSamplerAdvanced",
      "inputs": {
        "add_noise": "enable",
        "noise_seed": "{{seed}}",
        "steps": 4,
        "cfg": 1.0,
        "sampler_name": "euler",
        "scheduler": "simple",
        "start_at_step": 0,
        "end_at_step": 2,
        "return_with_leftover_noise": "enable",
        "model": ["10", 0],
        "positive": ["6", 0],
        "negative": ["7", 0],
        "latent_image": ["5", 0]
      },
      "_meta": {
        "title": "KSampler (Advanced) - High Noise"
      }
    },
    "13": {
      "class_type": "KSamplerAdvanced",
      "inputs": {
        "add_noise": "disable",
        "noise_seed": "{{seed}}",
        "steps": 4,
        "cfg": 1.0,
        "sampler_name": "euler",
        "scheduler": "simple",
        "start_at_step": 2,
        "end_at_step": 4,
        "return_with_leftover_noise": "disable",
        "model": ["11", 0],
        "positive": ["6", 0],
        "negative": ["7", 0],
        "latent_image": ["12", 0]
      },
      "_meta": {
        "title": "KSampler (Advanced) - Low Noise"
      }
    },
    "14": {
      "class_type": "VAEDecode",
      "inputs": {
        "samples": ["13", 0],
        "vae": ["2", 0]
      },
      "_meta": {
        "title": "VAE Decode"
      }
    },
    "15": {
      "class_type": "CreateVideo",
      "inputs": {
        "fps": "{{frame_rate}}",
        "images": ["14", 0]
      },
      "_meta": {
        "title": "Create Video"
      }
    },
    "16": {
      "class_type": "SaveVideo",
      "inputs": {
        "filename_prefix": "video/ComfyChair",
        "format": "auto",
        "codec": "auto",
        "video-preview": "",
        "video": ["15", 0]
      },
      "_meta": {
        "title": "Save Video"
      }
    }
  }
}
